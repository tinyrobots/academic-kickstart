+++
title = "Talk at the Alexander von Humboldt network meeting"

# Schedule page publish date (NOT talk date).
publishDate = 2017-01-01T00:00:00

# Authors. Comma separated list, e.g. `["Bob Smith", "David Jones"]`.
authors = ["Katherine R. Storrs"]

# Talk start and end times.
#   End time can optionally be hidden by prefixing the line with `#`.
date = 2020-03-01T12:00:00
date_end = 2020-03-01T18:00:00
all_day = false

# Location of event.
location = "University of Cologne"

# Name of event and optional event URL.
event = "How Brains and Machines Learn to See 'Stuff'"
event_url = "https://www.kukm-conferences.com/Home/Index/Event/avhnet2020koeln/en-GB"

# Abstract. What's your talk about?
abstract = "How does the brain learn to see? Newborn babies can hardly recognize anything by sight, yet by the time we are adults we can make exquisitely fine judgments about an object's 3D shape, whether it is soft or hard, fragile or durable, and anticipate how it is likely to feel if we touch it. Somehow, by looking at and interacting with lots of 'Stuff,' we learn how to recognize it. Meanwhile, computer vision has made astounding progress in the past decade through innovations in building and training deep neural networks (DNNs). However, most state-of-the-art achievements rely on supervised learning, which requires huge labelled training datasets. As a model of biological vision, these networks make the fundamentally impossible assumption that the brain has access to the ground-truth state of the physical world, and simply learns a correspondence between sensory data and those true states. I am interested in an alternative and more ecologically plausible kind of deep learning – unsupervised learning – in which DNNs learn the statistical structure of images, without any additional scene information. I will talk about two projects in which I am using unsupervised deep learning, combined with large computer-rendered stimulus sets, as a framework to understand how brains learn rich scene representations without ground-truth world information. Strikingly, the unsupervised models not only learn to represent physical properties such as shape, illumination, and material, but also suffer similar patterns of errors and 'visual illusions' as human observers do. I am excited by the possibility that unsupervised learning principles may account for a large number of perceptual dimensions in vision and beyond."

# Summary. An optional shortened abstract.
summary = ""

# Is this a featured talk? (true/false)
featured = true

# Tags (optional).
#   Set `tags = []` for no tags, or use the form `tags = ["A Tag", "Another Tag"]` for one or more tags.
tags = ["display"]

# Markdown Slides (optional).
#   Associate this talk with Markdown slides.
#   Simply enter your slide deck's filename without extension.
#   E.g. `slides = "example-slides"` references 
#   `content/slides/example-slides.md`.
#   Otherwise, set `slides = ""`.
slides = ""

# Optional filename of your slides within your talk folder or a URL.
url_slides = ""

# Projects (optional).
#   Associate this talk with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["deep-learning"]` references 
#   `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects = []

# Links (optional).
url_pdf = ""
url_video = ""
url_code = ""

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder. 
[image]
  # Caption (optional)
  caption = ""

  # Focal point (optional)
  # Options: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight
  focal_point = "Left"
+++
