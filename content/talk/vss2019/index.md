+++
title = "Unsupervised Neural Networks Learn Idiosyncrasies of Human Gloss Perception"

# Schedule page publish date (NOT talk date).
publishDate = 2017-01-01T00:00:00

# Authors. Comma separated list, e.g. `["Bob Smith", "David Jones"]`.
authors = ["Katherine R. Storrs", "Roland W. Fleming"]

# Talk start and end times.
#   End time can optionally be hidden by prefixing the line with `#`.
date = 2019-05-21T12:00:00
date_end = 2019-05-21T12:30:00
all_day = false

# Location of event.
location = "Vision Sciences Society conference, St. Pete Beach, Florida"

# Name of event and optional event URL.
event = "Conference Talk"
event_url = ""

# Abstract. What's your talk about?
abstract = "We suggest that characteristic errors in gloss perception may result from the way we learn vision more generally, through unsupervised objectives such as efficiently encoding and predicting proximal image data. We tested this idea using unsupervised deep learning. We rendered 10,000 images of bumpy surfaces of random albedos, bump heights, and illuminations, half with high-gloss and half with low-gloss reflectance. We trained an unsupervised six-layer convolutional PixelVAE network to generate new images based on this dataset. This network learns to predict pixel values given the previous ones, via a highly compressed 20-dimensional latent representation, and receives no information about gloss, shape or lighting during training. After training, multidimensional scaling revealed that the latent space substantially disentangles high and low gloss images. Indeed, a linear classifier could classify novel images with 97% accuracy. Samples generated by the network look like realistic surfaces, and their apparent glossiness can be modulated by moving systematically through the latent space. We generated 30 such images, and asked 11 observers to rate glossiness. Ratings correlated moderately well with gloss predictions from the model (_r_ = 0.64). To humans, perceived gloss can be modulated by lighting and surface shape. When the model was shown sequences varying only in bump height, it frequently exhibited non-constant gloss values, most commonly, like humans, seeing flatter surfaces as less glossy. We tested whether these failures of constancy in the model matched human perception for specific stimulus sequences. Nine observers viewed triads of images from five sequences in a Maximum Likelihood Difference Scaling experiment. Gloss values within the unsupervised model better predicted human perceptual gloss scales than did pixelwise differences between images (_p_= 0.042). Unsupervised machine learning may hold the key to fully explicit, image-computable models of how we learn meaningful perceptual dimensions from natural data."

# Summary. An optional shortened abstract.
summary = ""

# Is this a featured talk? (true/false)
featured = true

# Tags (optional).
#   Set `tags = []` for no tags, or use the form `tags = ["A Tag", "Another Tag"]` for one or more tags.
tags = []

# Markdown Slides (optional).
#   Associate this talk with Markdown slides.
#   Simply enter your slide deck's filename without extension.
#   E.g. `slides = "example-slides"` references 
#   `content/slides/example-slides.md`.
#   Otherwise, set `slides = ""`.
slides = ""

# Optional filename of your slides within your talk folder or a URL.
url_slides = ""

# Projects (optional).
#   Associate this talk with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["deep-learning"]` references 
#   `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects = []

# Links (optional).
url_pdf = ""
url_video = ""
url_code = ""

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder. 
[image]
  # Caption (optional)
  caption = ""

  # Focal point (optional)
  # Options: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight
  focal_point = "Left"
+++
